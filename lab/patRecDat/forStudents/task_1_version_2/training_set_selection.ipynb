{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from copy import deepcopy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#boundary detection\n",
    "class training_set_selection_miniBatch_kmeans(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,labels,kmeans):\n",
    "        self.labels=labels\n",
    "        self.kmeans=kmeans\n",
    "    def fit(self,X,y=None):\n",
    "        start=time.time()\n",
    "        print(\"fit start\")\n",
    "        self.kmeans.fit(X)\n",
    "        print(\"fit end\")\n",
    "        print(\"fit time:%5.1fminute\"%((time.time()-start)/60))\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        #dataset=pd.concat([X,self.labels],axis=1)\n",
    "        count1=X.shape[0]\n",
    "        predict_cluster = self.kmeans.predict(X)\n",
    "        predict_cluster=pd.DataFrame( predict_cluster,columns=['predict_cluster'])\n",
    "        X=pd.concat([X,predict_cluster],axis=1)\n",
    "        self.labels=pd.concat([self.labels,predict_cluster],axis=1)\n",
    "        #center_set = pd.DataFrame(columns = ['feature0','feature1','feature2','feature3','feature4','predict_label'])\n",
    "        center_set = list()\n",
    "        i = 0\n",
    "        #print (X)\n",
    "        #print (self.kmeans.cluster_centers_)\n",
    "        print(\"drop start\")\n",
    "        for cluster in range(np.int((X.shape[0])/50)):\n",
    "            if cluster%1000==0:\n",
    "                print(\"cluster number:\",cluster)\n",
    "            temp_list = list(self.labels[self.labels['predict_cluster']==cluster]['label'])\n",
    "            if 1 and 2 in temp_list:\n",
    "                pass\n",
    "            elif temp_list == []:  \n",
    "                pass\n",
    "            else:\n",
    "                #temp =temp_list[0]\n",
    "                #temp_center = np.append(self.kmeans.cluster_centers_[cluster],temp)\n",
    "                temp_center_set = {'feature0':self.kmeans.cluster_centers_[cluster][0],\\\n",
    "                                  'feature1':self.kmeans.cluster_centers_[cluster][1],\\\n",
    "                                  'feature2':self.kmeans.cluster_centers_[cluster][2],\\\n",
    "                                  'feature3':self.kmeans.cluster_centers_[cluster][3],\\\n",
    "                                  'feature4':self.kmeans.cluster_centers_[cluster][4],\\\n",
    "                                  'predict_label':temp_list[0]}\n",
    "                #temp = pd.DataFrame(data=temp_center_set,index = cluster)\n",
    "                #center_set = pd.concat(center_set,temp_center_set)\n",
    "                center_set.append(temp_center_set)\n",
    "                self.labels = self.labels[self.labels.predict_cluster !=cluster]\n",
    "                X = X[X.predict_cluster != cluster]\n",
    "        #dataset.append(pd.DataFrame(self.kmeans.cluster_centers_))#save all the centers of abandoned clusters\n",
    "        print(\"drop end\")\n",
    "        count2=X.shape[0]\n",
    "        print(\"Drop count:\",count1-count2)        \n",
    "        return X,self.labels,center_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_result_after_selection (data,stuck,index,slices,features,selected):\n",
    "    cmap1 = matplotlib.colors.LinearSegmentedColormap.from_list('my_cmap',['black','green','blue'],256)\n",
    "    target_data=data[stuck][0][0][0][0][:,:,slices,features]\n",
    "    target_image1=target_data.reshape(target_data.shape[0],target_data.shape[1])\n",
    "    target_label=data[stuck][0][0][0][index][:,:,slices]\n",
    "    target_image2=target_label.reshape(target_label.shape[0],target_label.shape[1])\n",
    "    plt.imshow(target_image1,cmap = matplotlib.cm.binary,alpha=0.4)\n",
    "    plt.imshow(target_image2,cmap = cmap1,interpolation=\"bilinear\",alpha=0.4)\n",
    "    plt.axis(\"off\") #close the axis number\n",
    "\n",
    "    selected_x=[]\n",
    "    selected_y=[]\n",
    "    selected_matrix = selected[selected['sourceofpixel']==stuck]\n",
    "    selected_matrix=pd.DataFrame.reset_index(selected_matrix).drop('index',axis=1)\n",
    "    for i in range(len(selected_matrix)):\n",
    "        temparr = re.findall(\"\\d+\",selected_matrix.iloc[i]['indexofpixel']) \n",
    "        if (int(temparr[2]) == slices):\n",
    "            selected_x.append(int(temparr[0]))\n",
    "            selected_y.append(int(temparr[1]))\n",
    "    plt.scatter(selected_y,selected_x,color='r',label=\"selected boundary\",s = 3)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_set_selection_FCNN (X_train,y_train):\n",
    "    train = {'feature0':X_train['feature0'],'feature1':X_train['feature1'],'feature2':X_train['feature2'],'feature3':X_train['feature3'],'feature4':X_train['feature4'],'label':y_train['label'],'indexofpixel':y_train['indexofpixel'],'sourceofpixel':y_train['sourceofpixel']}\n",
    "    train = pd.DataFrame(data=train)\n",
    "    train = pd.DataFrame.sample(train,frac=1)\n",
    "    \n",
    "    start1=time.time()\n",
    "    #define how many data a group contains\n",
    "    data_size_per_slice = 10000\n",
    "    data_slice_number = math.ceil(len(train)/data_size_per_slice)\n",
    "    print(\"The tatal amount of data slices: %d\"%(data_slice_number ))\n",
    "    print(\"                           \")\n",
    "\n",
    "    for j in range(data_slice_number):\n",
    "        print(\"current data slice:\"+str(j))\n",
    "        train_slice = train[j*data_size_per_slice:(j+1)*data_size_per_slice]\n",
    "        train_slice = pd.DataFrame.reset_index(train_slice).drop('index',axis=1)\n",
    "\n",
    "        class1_indices=np.random.choice(train_slice[train_slice['label']==1].index,1,replace=False)\n",
    "        class2_indices=np.random.choice(train_slice[train_slice['label']==2].index,1,replace=False)\n",
    "        class1_sample = train_slice.iloc[class1_indices]\n",
    "        class2_sample = train_slice.iloc[class2_indices]\n",
    "        train_slice=train_slice.drop(class1_indices)\n",
    "        train_slice=train_slice.drop(class2_indices)\n",
    "        train_slice = pd.DataFrame.reset_index(train_slice).drop('index',axis=1)\n",
    "        class1_sample = pd.DataFrame.reset_index(class1_sample).drop('index',axis=1)\n",
    "        class2_sample = pd.DataFrame.reset_index(class2_sample).drop('index',axis=1)\n",
    "\n",
    "        Store=list()\n",
    "        newStore=list()\n",
    "        class1_sample= {'feature0':class1_sample.iloc[0]['feature0'],'feature1':class1_sample.iloc[0]['feature1'],'feature2':class1_sample.iloc[0]['feature2'],'feature3':class1_sample.iloc[0]['feature3'],'feature4':class1_sample.iloc[0]['feature4'],'label':class1_sample.iloc[0]['label'],'indexofpixel':class1_sample.iloc[0]['indexofpixel'],'sourceofpixel':class1_sample.iloc[0]['sourceofpixel']}\n",
    "        class2_sample= {'feature0':class2_sample.iloc[0]['feature0'],'feature1':class2_sample.iloc[0]['feature1'],'feature2':class2_sample.iloc[0]['feature2'],'feature3':class2_sample.iloc[0]['feature3'],'feature4':class2_sample.iloc[0]['feature4'],'label':class2_sample.iloc[0]['label'],'indexofpixel':class2_sample.iloc[0]['indexofpixel'],'sourceofpixel':class2_sample.iloc[0]['sourceofpixel']}\n",
    "        newStore.append(class1_sample)\n",
    "        newStore.append(class2_sample)\n",
    "\n",
    "        start=time.time()\n",
    "        count=0\n",
    "        while newStore:\n",
    "            count = count+1\n",
    "            oldlength = len(Store)\n",
    "            for element in newStore:\n",
    "                Store.append(element)\n",
    "\n",
    "            temp=pd.DataFrame(data=newStore)\n",
    "            temp=np.array(temp)\n",
    "            temp=temp[:,0:5]\n",
    "            np.cast[float](temp)\n",
    "\n",
    "            train_temp=np.array(train_slice)\n",
    "            train_temp=train_temp[:,0:5]\n",
    "\n",
    "            start1=time.time()\n",
    "            dis=distance.cdist(temp.reshape(len(newStore),5),train_temp,'euclidean')\n",
    "            nearest_q_matrix=dis.min(0)\n",
    "            nearest_q_indexmat=dis.argmin(axis=0)\n",
    "\n",
    "            #update the nearest distance of q in (T-S) dataset, and the corresponding p in S.\n",
    "            if count==1:\n",
    "                nearest_q={'p_index':nearest_q_indexmat,'nearest_dis':nearest_q_matrix}\n",
    "                nearest_q=pd.DataFrame(data=nearest_q)\n",
    "                nearest_q=pd.concat([nearest_q,train_slice['label']],axis=1,join='outer')\n",
    "            else:\n",
    "                nearest_q_indexmat=nearest_q_indexmat+oldlength\n",
    "                nearest_q_new={'p_index_new':nearest_q_indexmat,'nearest_dis_new':nearest_q_matrix}\n",
    "                nearest_q_new=pd.DataFrame(data=nearest_q_new)\n",
    "                nearest_q=pd.concat([nearest_q,nearest_q_new],axis=1,join='outer')\n",
    "                nearest_q['p_index'] = np.where((nearest_q['nearest_dis_new']<nearest_q['nearest_dis']),nearest_q['p_index_new'],nearest_q['p_index'])\n",
    "                nearest_q['nearest_dis'] = np.where((nearest_q['nearest_dis_new']<nearest_q['nearest_dis']),nearest_q['nearest_dis_new'],nearest_q['nearest_dis'])\n",
    "                nearest_q=nearest_q.drop('p_index_new',axis=1)\n",
    "                nearest_q=nearest_q.drop('nearest_dis_new',axis=1)\n",
    "\n",
    "            Store_dataframe=pd.DataFrame(data=Store)\n",
    "            #attach p's label to the dataframe of nearest_q\n",
    "            p_label=pd.DataFrame(Store_dataframe.iloc[nearest_q['p_index']]['label'])\n",
    "            p_label.columns=['p_label']\n",
    "            p_label=pd.DataFrame.reset_index(p_label).drop('index',axis=1)\n",
    "            nearest_q=pd.concat([nearest_q,p_label],axis=1,join='outer')\n",
    "\n",
    "            #choose those data  q whose label is not equal to the lapel of nearest_q\n",
    "            # and then creat the dataframe called different_predict_points\n",
    "            temp_setting=np.ones(len(train_slice))\n",
    "            temp_setting=pd.DataFrame(data=temp_setting)\n",
    "            temp_setting.columns=['setting']\n",
    "            nearest_q=pd.concat([nearest_q,temp_setting],axis=1,join='outer')\n",
    "            nearest_q['choose'] = np.where((nearest_q['label']!=nearest_q['p_label']),nearest_q['setting'],np.nan)\n",
    "\n",
    "            different_predic_points=pd.DataFrame(nearest_q[nearest_q['choose']==1])\n",
    "\n",
    "            nearest_q=nearest_q.drop('setting',axis=1)\n",
    "            nearest_q=nearest_q.drop('choose',axis=1)\n",
    "\n",
    "\n",
    "            newStore=list()\n",
    "\n",
    "            start2=time.time()\n",
    "            for i in range(len(Store)):\n",
    "                temp=different_predic_points[different_predic_points['p_index']==i]\n",
    "                temp=pd.DataFrame.reset_index(temp)\n",
    "                if len(temp)>0 :\n",
    "                    location=temp['nearest_dis'].idxmin(axis=1)\n",
    "                    add_positon=int(temp.loc[location]['index'])\n",
    "                    temp_newone= {'feature0':train_slice.loc[add_positon]['feature0'],'feature1':train_slice.loc[add_positon]['feature1'],'feature2':train_slice.loc[add_positon]['feature2'],'feature3':train_slice.loc[add_positon]['feature3'],'feature4':train_slice.loc[add_positon]['feature4'],'label':train_slice.loc[add_positon]['label'],'indexofpixel':train_slice.loc[add_positon]['indexofpixel'],'sourceofpixel':train_slice.loc[add_positon]['sourceofpixel']}\n",
    "                    newStore.append(temp_newone)\n",
    "                    train_slice=train_slice.drop(add_positon)\n",
    "                    nearest_q=nearest_q.drop(add_positon)\n",
    "\n",
    "            nearest_q=nearest_q.drop('p_label',axis=1)\n",
    "            train_slice=pd.DataFrame.reset_index(train_slice).drop('index',axis=1)\n",
    "            nearest_q=pd.DataFrame.reset_index(nearest_q).drop('index',axis=1)\n",
    "        print(\"current count:%d current size of store:%d time:%5.1f minute\"%(count,len(Store),(time.time()-start)/60))\n",
    "\n",
    "        Store=pd.DataFrame(data=Store)\n",
    "        if (j==0):\n",
    "            Store_total=Store\n",
    "        else:\n",
    "            frame=[Store_total,Store]\n",
    "            Store_total=pd.concat(frame)\n",
    "            Store_total=pd.DataFrame.reset_index(Store_total).drop('index',axis=1)\n",
    "    print(\"The total execution time:%5.1f minute\"%((time.time()-start1)/60))\n",
    "    \n",
    "    Store_total_feature=Store_total.loc[:,['feature0','feature1','feature2','feature3','feature4']]\n",
    "    Store_total_label = Store_total.loc[:,['indexofpixel','label','sourceofpixel']]\n",
    "    return Store_total_feature,Store_total_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_set_selection_CNN (X_train,y_train):\n",
    "    train = {'feature0':X_train['feature0'],'feature1':X_train['feature1'],'feature2':X_train['feature2'],'feature3':X_train['feature3'],'feature4':X_train['feature4'],'label':y_train['label'],'indexofpixel':y_train['indexofpixel'],'sourceofpixel':y_train['sourceofpixel']}\n",
    "    train = pd.DataFrame(data=train)\n",
    "    train = pd.DataFrame.sample(train,frac=1)\n",
    "    \n",
    "    data_size_per_slice = 10000\n",
    "    data_slice_number = math.ceil(len(train)/data_size_per_slice)\n",
    "    print(\"The tatal amount of data slices: %d\"%(data_slice_number ))\n",
    "    print(\"                           \")\n",
    "    drop=0\n",
    "    start1=time.time()\n",
    "\n",
    "    for j in range(data_slice_number):\n",
    "        #print (j)\n",
    "        print(\"current data slice:\"+str(j))\n",
    "        data_slice = train[j*data_size_per_slice:(j+1)*data_size_per_slice]\n",
    "        train_feature = data_slice[['feature0','feature1','feature2','feature3','feature4']]\n",
    "        train_label = data_slice[['indexofpixel','label','sourceofpixel']]\n",
    "        train_feature=pd.DataFrame.reset_index(train_feature).drop('index',axis=1)\n",
    "        train_label=pd.DataFrame.reset_index(train_label).drop('index',axis=1)\n",
    "        Store_feature = train_feature\n",
    "        Store_label = train_label\n",
    "        count=1\n",
    "        drop_temp=0\n",
    "        start=time.time()\n",
    "        for i in range(len(train_feature)-1):\n",
    "            sample=np.array(train_feature.iloc[i+1]).reshape(1,-1)\n",
    "            temp = np.array(Store_feature[:count])\n",
    "            m_dis=np.array(distance.cdist(temp,sample,metric='euclidean'))\n",
    "            m_dis =list(m_dis)\n",
    "            min_index=m_dis.index(min(m_dis))\n",
    "            if (int(Store_label.iloc[min_index]['label'])!=int(train_label.iloc[i]['label'])):\n",
    "                count=count+1\n",
    "            else:\n",
    "                drop_temp=drop_temp+1\n",
    "                Store_feature=Store_feature.drop(count)\n",
    "                Store_label=Store_label.drop(count)\n",
    "                Store_feature = pd.DataFrame.reset_index(Store_feature).drop('index',axis=1)\n",
    "                Store_label = pd.DataFrame.reset_index(Store_label).drop('index',axis=1)       \n",
    "        print(\"During this iteratiron, %d data is discarded\"%(drop_temp))\n",
    "        print(\"this interation costs time:%5.1f minute\"%((time.time()-start)/60))\n",
    "        print(\"              \")\n",
    "        if (j==0):\n",
    "            drop=drop+drop_temp\n",
    "            Store_feature_total=Store_feature\n",
    "            Store_label_total=Store_label\n",
    "        else:\n",
    "            drop=drop+drop_temp\n",
    "            frame=[Store_feature_total,Store_feature]\n",
    "            Store_feature_total=pd.concat(frame)\n",
    "            Store_feature_total = pd.DataFrame.reset_index(Store_feature_total).drop('index',axis=1)\n",
    "            frame_2=[Store_label_total,Store_label]\n",
    "            Store_label_total=pd.concat(frame_2)\n",
    "            Store_label_total = pd.DataFrame.reset_index(Store_label_total).drop('index',axis=1)\n",
    "    \n",
    "    print(\"The total executing time:%5.1f minute\"%((time.time()-start1)/60))\n",
    "    print(\"The total discarded data: %d\"%(drop))\n",
    "    return Store_feature_total,Store_label_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
