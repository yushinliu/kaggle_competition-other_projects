{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.with drop LB_0.271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = \"C:\\\\Users\\\\hasee\\\\workspace\\\\Kaggle\\\\safe_driver\\\\\" # your folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0c80fb3a44fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m '''\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_path' is not defined"
     ]
    }
   ],
   "source": [
    "''''This script was inspired by snowdog's R kernel that builds an old school neural network \n",
    "   which scores quite well on the public LB relative to other NN approaches.\n",
    "   https://www.kaggle.com/snowdog/old-school-nnet\n",
    "\n",
    "   The idea is that after some pre-processing, a simpler network structure may generalize\n",
    "   much better than a deep, complicated one. The network in this script has only 1 hidden layer\n",
    "   with 35 neurons, uses some dropout, and trains for just 15 epochs. \n",
    "   Upsampling is also used, which seems to improve NN results. \n",
    "   \n",
    "   We'll do a 5-fold split on the data, train 3 times on each fold and bag the predictions, then average\n",
    "   the bagged predictions to get a submission. Increasing the number of training folds and the\n",
    "   number of runs per fold would likely improve the results.\n",
    "   \n",
    "   The LB score is approximate because I haven't been able to get random seeding to properly\n",
    "   make keras results consistent - any advice here would be much appreciated! \n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(20)\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "'''Data loading & preprocessing\n",
    "'''\n",
    "\n",
    "X_train = pd.read_csv(base_path+'train.csv')\n",
    "X_test = pd.read_csv(base_path+'test.csv')\n",
    "\n",
    "X_train, y_train = X_train.iloc[:,2:], X_train.target\n",
    "X_test, test_id = X_test.iloc[:,1:], X_test.id\n",
    "\n",
    "#OHE / some feature engineering adapted from the1owl kernel at:\n",
    "#https://www.kaggle.com/the1owl/forza-baseline/code\n",
    "\n",
    "#excluded columns based on snowdog's old school nn kernel at:\n",
    "#https://www.kaggle.com/snowdog/old-school-nnet\n",
    "\n",
    "X_train['negative_one_vals'] = np.sum((X_train==-1).values, axis=1)\n",
    "X_test['negative_one_vals'] = np.sum((X_test==-1).values, axis=1)\n",
    "\n",
    "to_drop = ['ps_car_11_cat', 'ps_ind_14', 'ps_car_11', 'ps_car_14', 'ps_ind_06_bin', \n",
    "           'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', \n",
    "           'ps_ind_13_bin']\n",
    "\n",
    "cols_use = [c for c in X_train.columns if (not c.startswith('ps_calc_'))\n",
    "             & (not c in to_drop)]\n",
    "             \n",
    "X_train = X_train[cols_use]\n",
    "X_test = X_test[cols_use]\n",
    "\n",
    "one_hot = {c: list(X_train[c].unique()) for c in X_train.columns}\n",
    "\n",
    "#note that this encodes the negative_one_vals column as well\n",
    "for c in one_hot:\n",
    "    if len(one_hot[c])>2 and len(one_hot[c]) < 105:\n",
    "        for val in one_hot[c]:\n",
    "            newcol = c + '_oh_' + str(val)\n",
    "            X_train[newcol] = (X_train[c].values == val).astype(np.int)\n",
    "            X_test[newcol] = (X_test[c].values == val).astype(np.int)\n",
    "        X_train.drop(labels=[c], axis=1, inplace=True)\n",
    "        X_test.drop(labels=[c], axis=1, inplace=True)\n",
    "            \n",
    "X_train = X_train.replace(-1, np.NaN)  # Get rid of -1 while computing interaction col\n",
    "X_test = X_test.replace(-1, np.NaN)\n",
    "\n",
    "X_train['ps_car_13_x_ps_reg_03'] = X_train['ps_car_13'] * X_train['ps_reg_03']\n",
    "X_test['ps_car_13_x_ps_reg_03'] = X_test['ps_car_13'] * X_test['ps_reg_03']\n",
    "\n",
    "X_train = X_train.fillna(-1)\n",
    "X_test = X_test.fillna(-1)\n",
    "\n",
    "'''Gini scoring function\n",
    "'''\n",
    "\n",
    "#gini scoring function from kernel at: \n",
    "#https://www.kaggle.com/tezdhar/faster-gini-calculation\n",
    "def ginic(actual, pred):\n",
    "    n = len(actual)\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_c[-1] - (n + 1) / 2.0\n",
    "    return giniSum / n\n",
    " \n",
    "def gini_normalizedc(a, p):\n",
    "    return ginic(a, p) / ginic(a, a)\n",
    "\n",
    "'''5-fold neural network training \n",
    "'''\n",
    "\n",
    "K = 5 #number of folds\n",
    "runs_per_fold = 3 #bagging on each fold\n",
    "\n",
    "cv_ginis = []\n",
    "y_preds = np.zeros((np.shape(X_test)[0],K))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = K, \n",
    "                            random_state = 100, \n",
    "                            shuffle = True)    \n",
    "\n",
    "for i, (f_ind, outf_ind) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "    X_train_f, X_val_f = X_train.loc[f_ind].copy(), X_train.loc[outf_ind].copy()\n",
    "    y_train_f, y_val_f = y_train[f_ind], y_train[outf_ind]\n",
    "          \n",
    "    #upsampling adapted from kernel: \n",
    "    #https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "    pos = (pd.Series(y_train_f == 1))\n",
    "    \n",
    "    # Add positive examples\n",
    "    X_train_f = pd.concat([X_train_f, X_train_f.loc[pos]], axis=0)\n",
    "    y_train_f = pd.concat([y_train_f, y_train_f.loc[pos]], axis=0)\n",
    "    \n",
    "    # Shuffle data\n",
    "    idx = np.arange(len(X_train_f))\n",
    "    np.random.shuffle(idx)\n",
    "    X_train_f = X_train_f.iloc[idx]\n",
    "    y_train_f = y_train_f.iloc[idx]\n",
    "    \n",
    "    #track oof bagged prediction for cv scores\n",
    "    val_preds = 0\n",
    "    \n",
    "    for j in range(runs_per_fold):\n",
    "    \n",
    "        NN=Sequential()\n",
    "        NN.add(Dense(35,activation='relu',input_dim=np.shape(X_train_f)[1]))\n",
    "        NN.add(Dropout(0.3))\n",
    "        NN.add(Dense(1,activation='sigmoid'))\n",
    "        \n",
    "        NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        set_random_seed(1000*i+j)\n",
    "            \n",
    "        NN.fit(X_train_f.values, y_train_f.values, epochs=15, batch_size=2048, verbose=0)\n",
    "         \n",
    "        val_gini = gini_normalizedc(y_val_f.values, NN.predict(X_val_f.values)[:,0])   \n",
    "        print ('\\nFold %d Run %d Results *****' % (i, j))\n",
    "        print ('Validation gini: %.5f\\n' % (val_gini))\n",
    "        \n",
    "        val_preds += NN.predict(X_val_f.values)[:,0] / runs_per_fold\n",
    "        y_preds[:,i] += NN.predict(X_test.values)[:,0] / runs_per_fold\n",
    "        \n",
    "    cv_ginis.append(val_gini)\n",
    "    print ('\\nFold %i prediction cv gini: %.5f\\n' %(i,val_gini))\n",
    "    \n",
    "print('Mean out of fold gini: %.5f' % np.mean(cv_ginis))\n",
    "y_pred_final = np.mean(y_preds, axis=1)\n",
    "\n",
    "df_sub = pd.DataFrame({'id' : test_id, \n",
    "                       'target' : y_pred_final},\n",
    "                       columns = ['id','target'])\n",
    "df_sub.to_csv(base_path+\"\\\\bagging\\\\NNShallow_5fold_3runs_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LB 0.272 after drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_unit is  33 dropout_rate is  0.2 optimizer is  SGD L2 is  0.001\n",
      "\n",
      "Fold 0 Run 0 Results *****\n",
      "Validation gini: 0.19061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''''This script was inspired by snowdog's R kernel that builds an old school neural network \n",
    "   which scores quite well on the public LB relative to other NN approaches.\n",
    "   https://www.kaggle.com/snowdog/old-school-nnet\n",
    "\n",
    "   The idea is that after some pre-processing, a simpler network structure may generalize\n",
    "   much better than a deep, complicated one. The network in this script has only 1 hidden layer\n",
    "   with 35 neurons, uses some dropout, and trains for just 15 epochs. \n",
    "   Upsampling is also used, which seems to improve NN results. \n",
    "   \n",
    "   We'll do a 5-fold split on the data, train 3 times on each fold and bag the predictions, then average\n",
    "   the bagged predictions to get a submission. Increasing the number of training folds and the\n",
    "   number of runs per fold would likely improve the results.\n",
    "   \n",
    "   The LB score is approximate because I haven't been able to get random seeding to properly\n",
    "   make keras results consistent - any advice here would be much appreciated! \n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(20)\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "'''Data loading & preprocessing\n",
    "'''\n",
    "\n",
    "X_train = pd.read_csv(base_path+'train.csv')\n",
    "X_test = pd.read_csv(base_path+'test.csv')\n",
    "\n",
    "X_train, y_train = X_train.iloc[:,2:], X_train.target\n",
    "X_test, test_id = X_test.iloc[:,1:], X_test.id\n",
    "\n",
    "#OHE / some feature engineering adapted from the1owl kernel at:\n",
    "#https://www.kaggle.com/the1owl/forza-baseline/code\n",
    "\n",
    "#excluded columns based on snowdog's old school nn kernel at:\n",
    "#https://www.kaggle.com/snowdog/old-school-nnet\n",
    "\n",
    "X_train['negative_one_vals'] = np.sum((X_train==-1).values, axis=1)\n",
    "X_test['negative_one_vals'] = np.sum((X_test==-1).values, axis=1)\n",
    "\n",
    "to_drop = ['ps_car_11_cat', 'ps_ind_14', 'ps_car_11', 'ps_car_14', 'ps_ind_06_bin', \n",
    "           'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', \n",
    "           'ps_ind_13_bin']\n",
    "\n",
    "cols_use = [c for c in X_train.columns if (not c.startswith('ps_calc_'))\n",
    "             & (not c in to_drop)]\n",
    "             \n",
    "X_train = X_train[cols_use]\n",
    "X_test = X_test[cols_use]\n",
    "\n",
    "one_hot = {c: list(X_train[c].unique()) for c in X_train.columns}\n",
    "\n",
    "#note that this encodes the negative_one_vals column as well\n",
    "for c in one_hot:\n",
    "    if len(one_hot[c])>2 and len(one_hot[c]) < 105:\n",
    "        for val in one_hot[c]:\n",
    "            newcol = c + '_oh_' + str(val)\n",
    "            X_train[newcol] = (X_train[c].values == val).astype(np.int)\n",
    "            X_test[newcol] = (X_test[c].values == val).astype(np.int)\n",
    "        #X_train.drop(labels=[c], axis=1, inplace=True)\n",
    "        #X_test.drop(labels=[c], axis=1, inplace=True)\n",
    "            \n",
    "X_train = X_train.replace(-1, np.NaN)  # Get rid of -1 while computing interaction col\n",
    "X_test = X_test.replace(-1, np.NaN)\n",
    "\n",
    "X_train['ps_car_13_x_ps_reg_03'] = X_train['ps_car_13'] * X_train['ps_reg_03']\n",
    "X_test['ps_car_13_x_ps_reg_03'] = X_test['ps_car_13'] * X_test['ps_reg_03']\n",
    "\n",
    "X_train = X_train.fillna(-1)\n",
    "X_test = X_test.fillna(-1)\n",
    "\n",
    "'''Gini scoring function\n",
    "'''\n",
    "\n",
    "#gini scoring function from kernel at: \n",
    "#https://www.kaggle.com/tezdhar/faster-gini-calculation\n",
    "def ginic(actual, pred):\n",
    "    n = len(actual)\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_c[-1] - (n + 1) / 2.0\n",
    "    return giniSum / n\n",
    " \n",
    "def gini_normalizedc(a, p):\n",
    "    return ginic(a, p) / ginic(a, a)\n",
    "\n",
    "'''5-fold neural network training \n",
    "'''\n",
    "\n",
    "K = 5 #number of folds\n",
    "runs_per_fold = 3 #bagging on each fold\n",
    "\n",
    "cv_ginis = []\n",
    "y_preds = np.zeros((np.shape(X_test)[0],K))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = K, \n",
    "                            random_state = 100, \n",
    "                            shuffle = True)    \n",
    "\n",
    "#add grid search to test\n",
    "for hidden_unit in [33,34,35,36,37]:\n",
    "    for dropout_rate in [0.2,0.25,0.3,0.35]:\n",
    "        for optimizer in ['SGD', 'Adagrad', 'Adadelta', 'Adam', 'Adamax']:\n",
    "            for L2 in [.001,.002,.003]:\n",
    "                print(\"hidden_unit is \",hidden_unit,\"dropout_rate is \",\\\n",
    "                      dropout_rate,\"optimizer is \",optimizer,\"L2 is \",L2)\n",
    "\n",
    "                for i, (f_ind, outf_ind) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "                    X_train_f, X_val_f = X_train.loc[f_ind].copy(), X_train.loc[outf_ind].copy()\n",
    "                    y_train_f, y_val_f = y_train[f_ind], y_train[outf_ind]\n",
    "          \n",
    "                  #upsampling adapted from kernel: \n",
    "                   #https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "                    pos = (pd.Series(y_train_f == 1))\n",
    "     \n",
    "                  # Add positive examples\n",
    "                    X_train_f = pd.concat([X_train_f, X_train_f.loc[pos]], axis=0)\n",
    "                    y_train_f = pd.concat([y_train_f, y_train_f.loc[pos]], axis=0)\n",
    "    \n",
    "                  # Shuffle data\n",
    "                    idx = np.arange(len(X_train_f))\n",
    "                    np.random.shuffle(idx)\n",
    "                    X_train_f = X_train_f.iloc[idx]\n",
    "                    y_train_f = y_train_f.iloc[idx]\n",
    "    \n",
    "                  #track oof bagged prediction for cv scores\n",
    "                    val_preds = 0\n",
    "    \n",
    "                    for j in range(runs_per_fold):\n",
    "    \n",
    "                        NN=Sequential()\n",
    "                        NN.add(Dense(hidden_unit,activation='relu',input_dim=np.shape(X_train_f)[1],kernel_regularizer=regularizers.l2(L2)))\n",
    "                        NN.add(Dropout(dropout_rate))\n",
    "                        NN.add(Dense(1,activation='sigmoid',kernel_regularizer=regularizers.l2(L2)))\n",
    "        \n",
    "                        NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "                        set_random_seed(1000*i+j)\n",
    "            \n",
    "                        NN.fit(X_train_f.values, y_train_f.values, epochs=15, batch_size=2048, verbose=0)\n",
    "         \n",
    "                        val_gini = gini_normalizedc(y_val_f.values, NN.predict(X_val_f.values)[:,0])   \n",
    "                        print ('\\nFold %d Run %d Results *****' % (i, j))\n",
    "                        print ('Validation gini: %.5f\\n' % (val_gini))\n",
    "        \n",
    "                        val_preds += NN.predict(X_val_f.values)[:,0] / runs_per_fold\n",
    "                        y_preds[:,i] += NN.predict(X_test.values)[:,0] / runs_per_fold\n",
    "        \n",
    "                    cv_ginis.append(val_gini)\n",
    "                    print ('\\nFold %i prediction cv gini: %.5f\\n' %(i,val_gini))\n",
    "    \n",
    "                print('Mean out of fold gini: %.5f' % np.mean(cv_ginis))\n",
    "#y_pred_final = np.mean(y_preds, axis=1)\n",
    "\n",
    "#df_sub = pd.DataFrame({'id' : test_id, \n",
    "                       #'target' : y_pred_final},\n",
    "                       #columns = ['id','target'])\n",
    "#df_sub.to_csv(base_path+\"\\\\bagging\\\\NNShallow_drop_5fold_3runs_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(20)\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from sklearn.grid_search import GridSearchCV  \n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''Data loading & preprocessing\n",
    "'''\n",
    "\n",
    "X_train = pd.read_csv(base_path+'train.csv')\n",
    "X_test = pd.read_csv(base_path+'test.csv')\n",
    "\n",
    "X_train, y_train = X_train.iloc[:,2:], X_train.target\n",
    "X_test, test_id = X_test.iloc[:,1:], X_test.id\n",
    "\n",
    "#OHE / some feature engineering adapted from the1owl kernel at:\n",
    "#https://www.kaggle.com/the1owl/forza-baseline/code\n",
    "\n",
    "#excluded columns based on snowdog's old school nn kernel at:\n",
    "#https://www.kaggle.com/snowdog/old-school-nnet\n",
    "\n",
    "X_train['negative_one_vals'] = np.sum((X_train==-1).values, axis=1)\n",
    "X_test['negative_one_vals'] = np.sum((X_test==-1).values, axis=1)\n",
    "\n",
    "to_drop = ['ps_car_11_cat', 'ps_ind_14', 'ps_car_11', 'ps_car_14', 'ps_ind_06_bin', \n",
    "           'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', \n",
    "           'ps_ind_13_bin']\n",
    "\n",
    "cols_use = [c for c in X_train.columns if (not c.startswith('ps_calc_'))\n",
    "             & (not c in to_drop)]\n",
    "             \n",
    "X_train = X_train[cols_use]\n",
    "X_test = X_test[cols_use]\n",
    "\n",
    "one_hot = {c: list(X_train[c].unique()) for c in X_train.columns}\n",
    "\n",
    "#note that this encodes the negative_one_vals column as well\n",
    "for c in one_hot:\n",
    "    if len(one_hot[c])>2 and len(one_hot[c]) < 105:\n",
    "        for val in one_hot[c]:\n",
    "            newcol = c + '_oh_' + str(val)\n",
    "            X_train[newcol] = (X_train[c].values == val).astype(np.int)\n",
    "            X_test[newcol] = (X_test[c].values == val).astype(np.int)\n",
    "        #X_train.drop(labels=[c], axis=1, inplace=True)\n",
    "        #X_test.drop(labels=[c], axis=1, inplace=True)\n",
    "            \n",
    "X_train = X_train.replace(-1, np.NaN)  # Get rid of -1 while computing interaction col\n",
    "X_test = X_test.replace(-1, np.NaN)\n",
    "\n",
    "X_train['ps_car_13_x_ps_reg_03'] = X_train['ps_car_13'] * X_train['ps_reg_03']\n",
    "X_test['ps_car_13_x_ps_reg_03'] = X_test['ps_car_13'] * X_test['ps_reg_03']\n",
    "\n",
    "X_train = X_train.fillna(-1)\n",
    "X_test = X_test.fillna(-1)\n",
    "\n",
    "'''Gini scoring function\n",
    "'''\n",
    "\n",
    "#gini scoring function from kernel at: \n",
    "#https://www.kaggle.com/tezdhar/faster-gini-calculation\n",
    "def ginic(actual, pred):\n",
    "    n = len(actual)\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_c[-1] - (n + 1) / 2.0\n",
    "    return giniSum / n\n",
    " \n",
    "def gini_normalizedc(a, p):\n",
    "    return ginic(a, p) / ginic(a, a)\n",
    "\n",
    "'''5-fold neural network training \n",
    "'''\n",
    "\n",
    "K = 5 #number of folds\n",
    "runs_per_fold = 3 #bagging on each fold\n",
    "\n",
    "cv_ginis = []\n",
    "y_preds = np.zeros((np.shape(X_test)[0],K))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = K, \n",
    "                            random_state = 100, \n",
    "                            shuffle = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_model(hidden_unit=0,dropout_rate=0,optimizer='adam',L2=0):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(hidden_unit,activation='relu',input_dim=X_train.shape[1],kernel_regularizer=l2(L2_REG)))\n",
    "    model.add(BatchNormalization((hidden_unit,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation='relu',kernel_regularizer=l2(L2_REG)))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy',gini_normalizedc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid={\n",
    "    'hidden_unit':[33,34,35,36,37],\n",
    "    'dropout_rate':[0.2,0.25,0.3,0.35],\n",
    "    'optimizer':['SGD', 'Adagrad', 'Adadelta', 'Adam', 'Adamax'],\n",
    "    'L2':[.001,.002,.003]\n",
    "}\n",
    "knn_clf=KerasClassifier(build_fn=NN_model,nb_epoch=15, batch_size=2048, verbose=-1)\n",
    "grid_clf=GridSearchCV(estimator=knn_clf, param_grid=param_grid, cv=3,n_jobs=-1)\n",
    "grid_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
