{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = \"C:\\\\Users\\\\hasee\\\\workspace\\\\Kaggle\\\\safe_driver\\\\\" # your folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEMO_MODE = True   # Warning: takes 2 days on my GTX1080 with DEMO_MODE=False\n",
    "                   #   (But you can also adjust the folds and runs parameters directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed()\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, Callback\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class roc_auc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict_proba(self.x, verbose=0)\n",
    "        roc = roc_auc_score(self.y, y_pred)\n",
    "        logs['roc_auc'] = roc_auc_score(self.y, y_pred)\n",
    "        logs['norm_gini'] = ( roc_auc_score(self.y, y_pred) * 2 ) - 1\n",
    "\n",
    "        y_pred_val = self.model.predict_proba(self.x_val, verbose=0)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        logs['roc_auc_val'] = roc_auc_score(self.y_val, y_pred_val)\n",
    "        logs['norm_gini_val'] = ( roc_auc_score(self.y_val, y_pred_val) * 2 ) - 1\n",
    "\n",
    "        print('\\rroc_auc: %s - roc_auc_val: %s - norm_gini: %s - norm_gini_val: %s' % (str(round(roc,5)),str(round(roc_val,5)),str(round((roc*2-1),5)),str(round((roc_val*2-1),5))), end=10*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod(\n",
    "            (datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n",
    "              (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "def scale_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and test data path\n",
    "DATA_TRAIN_PATH = base_path+'train.csv'\n",
    "DATA_TEST_PATH = base_path+'test.csv'\n",
    "\n",
    "def load_data(path_train=DATA_TRAIN_PATH, path_test=DATA_TEST_PATH):\n",
    "    train_loader = pd.read_csv(path_train, dtype={'target': np.int8, 'id': np.int32})\n",
    "    train = train_loader.drop(['target', 'id'], axis=1)\n",
    "    train_labels = train_loader['target'].values\n",
    "    train_ids = train_loader['id'].values\n",
    "    print('\\n Shape of raw train data:', train.shape)\n",
    "\n",
    "    test_loader = pd.read_csv(path_test, dtype={'id': np.int32})\n",
    "    test = test_loader.drop(['id'], axis=1)\n",
    "    test_ids = test_loader['id'].values\n",
    "    print(' Shape of raw test data:', test.shape)\n",
    "\n",
    "    return train, train_labels, test, train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if DEMO_MODE:\n",
    "    folds = 2\n",
    "    runs = 1\n",
    "    STARTING_LR = 2e-3\n",
    "    LR_DECAY = 3e-5\n",
    "    PATIENCE = 5\n",
    "    MAX_EPOCHS = 20\n",
    "else:\n",
    "    folds = 10\n",
    "    runs = 5\n",
    "    STARTING_LR = 4e-3\n",
    "    LR_DECAY = 1e-4\n",
    "    PATIENCE = 12\n",
    "    MAX_EPOCHS = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_LL = 0\n",
    "cv_AUC = 0\n",
    "cv_gini = 0\n",
    "fpred = []\n",
    "avpred = []\n",
    "avreal = []\n",
    "avids = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Shape of raw train data: (595212, 57)\n",
      " Shape of raw test data: (892816, 57)\n",
      "\n",
      " Shape of processed train data: (595212, 227)\n",
      " Shape of processed test data: (892816, 227)\n"
     ]
    }
   ],
   "source": [
    "# Load data set and target values\n",
    "train, target, test, tr_ids, te_ids = load_data()\n",
    "n_train = train.shape[0]\n",
    "train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "col_to_drop = train.columns[train.columns.str.endswith('_cat')]\n",
    "col_to_dummify = train.columns[train.columns.str.endswith('_cat')].astype(str).tolist()\n",
    "\n",
    "for col in col_to_dummify:\n",
    "    dummy = pd.get_dummies(train_test[col].astype('category'))\n",
    "    columns = dummy.columns.astype(str).tolist()\n",
    "    columns = [col + '_' + w for w in columns]\n",
    "    dummy.columns = columns\n",
    "    train_test = pd.concat((train_test, dummy), axis=1)\n",
    "\n",
    "train_test.drop(col_to_dummify, axis=1, inplace=True)\n",
    "train_test_scaled, scaler = scale_data(train_test)\n",
    "train = train_test_scaled[:n_train, :]\n",
    "test = train_test_scaled[n_train:, :]\n",
    "print('\\n Shape of processed train data:', train.shape)\n",
    "print(' Shape of processed test data:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patience = PATIENCE\n",
    "batchsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1 - Run 1\n",
      "\n",
      "Train on 297606 samples, validate on 297606 samples\n",
      "Epoch 1/20\n",
      "roc_auc: 0.62581 - roc_auc_val: 0.61978 - norm_gini: 0.25161 - norm_gini_val: 0.23956          \n",
      "Epoch 00000: norm_gini_val improved from -inf to 0.23956, saving model to keras-kfold-run-01-v1-fold-01-run-01.check\n",
      " - 141s - loss: 0.2064 - acc: 0.9523 - val_loss: 0.1670 - val_acc: 0.9636\n",
      "Epoch 2/20\n",
      "roc_auc: 0.62349 - roc_auc_val: 0.61583 - norm_gini: 0.24698 - norm_gini_val: 0.23166          \n",
      "Epoch 00001: norm_gini_val did not improve\n",
      " - 124s - loss: 0.1655 - acc: 0.9636 - val_loss: 0.1601 - val_acc: 0.9636\n",
      "Epoch 3/20\n",
      "roc_auc: 0.62776 - roc_auc_val: 0.62541 - norm_gini: 0.25552 - norm_gini_val: 0.25081          \n",
      "Epoch 00002: norm_gini_val improved from 0.23956 to 0.25081, saving model to keras-kfold-run-01-v1-fold-01-run-01.check\n",
      " - 125s - loss: 0.1603 - acc: 0.9636 - val_loss: 0.1573 - val_acc: 0.9636\n",
      "Epoch 4/20\n",
      "roc_auc: 0.62224 - roc_auc_val: 0.61806 - norm_gini: 0.24448 - norm_gini_val: 0.23612          \n",
      "Epoch 00003: norm_gini_val did not improve\n",
      " - 119s - loss: 0.1586 - acc: 0.9636 - val_loss: 0.1567 - val_acc: 0.9636\n",
      "Epoch 5/20\n",
      "roc_auc: 0.62027 - roc_auc_val: 0.61684 - norm_gini: 0.24053 - norm_gini_val: 0.23367          \n",
      "Epoch 00004: norm_gini_val did not improve\n",
      " - 118s - loss: 0.1578 - acc: 0.9636 - val_loss: 0.1571 - val_acc: 0.9636\n",
      "Epoch 6/20\n",
      "roc_auc: 0.62281 - roc_auc_val: 0.62077 - norm_gini: 0.24561 - norm_gini_val: 0.24153          \n",
      "Epoch 00005: norm_gini_val did not improve\n",
      " - 118s - loss: 0.1576 - acc: 0.9636 - val_loss: 0.1564 - val_acc: 0.9636\n",
      "Epoch 7/20\n",
      "roc_auc: 0.62563 - roc_auc_val: 0.6204 - norm_gini: 0.25126 - norm_gini_val: 0.2408          \n",
      "Epoch 00006: norm_gini_val did not improve\n",
      " - 119s - loss: 0.1571 - acc: 0.9636 - val_loss: 0.1570 - val_acc: 0.9636\n",
      "Epoch 8/20\n",
      "roc_auc: 0.62446 - roc_auc_val: 0.6224 - norm_gini: 0.24892 - norm_gini_val: 0.24481          \n",
      "Epoch 00007: norm_gini_val did not improve\n",
      " - 119s - loss: 0.1569 - acc: 0.9636 - val_loss: 0.1564 - val_acc: 0.9636\n",
      "Epoch 00007: early stopping\n",
      "\n",
      " Fold 1 Run 1 Log-loss: 0.15365\n",
      " Fold 1 Run 1 AUC: 0.62541\n",
      " Fold 1 Run 1 normalized gini: 0.25081\n",
      "\n",
      " Fold 1 Log-loss: 0.15365\n",
      " Fold 1 AUC: 0.62541\n",
      " Fold 1 normalized gini: 0.25081\n",
      "\n",
      " Time taken: 0 hours 18 minutes and 49.48 seconds.\n",
      "\n",
      " Fold 2 - Run 1\n",
      "\n",
      "Train on 297606 samples, validate on 297606 samples\n",
      "Epoch 1/20\n",
      "roc_auc: 0.6241 - roc_auc_val: 0.61594 - norm_gini: 0.2482 - norm_gini_val: 0.23187          \n",
      "Epoch 00000: norm_gini_val improved from -inf to 0.23187, saving model to keras-kfold-run-01-v1-fold-02-run-01.check\n",
      " - 137s - loss: 0.2065 - acc: 0.9522 - val_loss: 0.1680 - val_acc: 0.9636\n",
      "Epoch 2/20\n",
      "roc_auc: 0.62647 - roc_auc_val: 0.61723 - norm_gini: 0.25294 - norm_gini_val: 0.23447          \n",
      "Epoch 00001: norm_gini_val improved from 0.23187 to 0.23447, saving model to keras-kfold-run-01-v1-fold-02-run-01.check\n",
      " - 130s - loss: 0.1658 - acc: 0.9635 - val_loss: 0.1596 - val_acc: 0.9636\n",
      "Epoch 3/20\n",
      "roc_auc: 0.62584 - roc_auc_val: 0.61845 - norm_gini: 0.25168 - norm_gini_val: 0.2369          \n",
      "Epoch 00002: norm_gini_val improved from 0.23447 to 0.23690, saving model to keras-kfold-run-01-v1-fold-02-run-01.check\n",
      " - 132s - loss: 0.1600 - acc: 0.9636 - val_loss: 0.1572 - val_acc: 0.9636\n",
      "Epoch 4/20\n",
      "roc_auc: 0.62172 - roc_auc_val: 0.61343 - norm_gini: 0.24344 - norm_gini_val: 0.22685          \n",
      "Epoch 00003: norm_gini_val did not improve\n",
      " - 129s - loss: 0.1581 - acc: 0.9636 - val_loss: 0.1574 - val_acc: 0.9636\n",
      "Epoch 5/20\n",
      "roc_auc: 0.62658 - roc_auc_val: 0.61614 - norm_gini: 0.25315 - norm_gini_val: 0.23229          \n",
      "Epoch 00004: norm_gini_val did not improve\n",
      " - 124s - loss: 0.1579 - acc: 0.9636 - val_loss: 0.1578 - val_acc: 0.9636\n",
      "Epoch 6/20\n",
      "roc_auc: 0.6293 - roc_auc_val: 0.62031 - norm_gini: 0.2586 - norm_gini_val: 0.24062          \n",
      "Epoch 00005: norm_gini_val improved from 0.23690 to 0.24062, saving model to keras-kfold-run-01-v1-fold-02-run-01.check\n",
      " - 129s - loss: 0.1575 - acc: 0.9636 - val_loss: 0.1558 - val_acc: 0.9636\n",
      "Epoch 7/20\n",
      "roc_auc: 0.62785 - roc_auc_val: 0.61954 - norm_gini: 0.2557 - norm_gini_val: 0.23907          \n",
      "Epoch 00006: norm_gini_val did not improve\n",
      " - 133s - loss: 0.1570 - acc: 0.9636 - val_loss: 0.1583 - val_acc: 0.9636\n",
      "Epoch 8/20\n",
      "roc_auc: 0.62885 - roc_auc_val: 0.61969 - norm_gini: 0.2577 - norm_gini_val: 0.23938          \n",
      "Epoch 00007: norm_gini_val did not improve\n",
      " - 128s - loss: 0.1566 - acc: 0.9636 - val_loss: 0.1557 - val_acc: 0.9636\n",
      "Epoch 9/20\n",
      "roc_auc: 0.62898 - roc_auc_val: 0.6221 - norm_gini: 0.25797 - norm_gini_val: 0.2442          \n",
      "Epoch 00008: norm_gini_val improved from 0.24062 to 0.24420, saving model to keras-kfold-run-01-v1-fold-02-run-01.check\n",
      " - 129s - loss: 0.1562 - acc: 0.9636 - val_loss: 0.1641 - val_acc: 0.9636\n",
      "Epoch 10/20\n",
      "roc_auc: 0.62705 - roc_auc_val: 0.62129 - norm_gini: 0.25409 - norm_gini_val: 0.24257          \n",
      "Epoch 00009: norm_gini_val did not improve\n",
      " - 128s - loss: 0.1561 - acc: 0.9636 - val_loss: 0.1566 - val_acc: 0.9636\n",
      "Epoch 11/20\n",
      "roc_auc: 0.62711 - roc_auc_val: 0.61917 - norm_gini: 0.25423 - norm_gini_val: 0.23834          \n",
      "Epoch 00010: norm_gini_val did not improve\n",
      " - 137s - loss: 0.1558 - acc: 0.9636 - val_loss: 0.1559 - val_acc: 0.9636\n",
      "Epoch 12/20\n",
      "roc_auc: 0.62897 - roc_auc_val: 0.62218 - norm_gini: 0.25794 - norm_gini_val: 0.24436          \n",
      "Epoch 00011: norm_gini_val improved from 0.24420 to 0.24436, saving model to keras-kfold-run-01-v1-fold-02-run-01.check\n",
      " - 141s - loss: 0.1558 - acc: 0.9636 - val_loss: 0.1555 - val_acc: 0.9636\n",
      "Epoch 13/20\n",
      "roc_auc: 0.62424 - roc_auc_val: 0.61206 - norm_gini: 0.24847 - norm_gini_val: 0.22411          \n",
      "Epoch 00012: norm_gini_val did not improve\n",
      " - 140s - loss: 0.1556 - acc: 0.9636 - val_loss: 0.1562 - val_acc: 0.9636\n",
      "Epoch 14/20\n",
      "roc_auc: 0.63247 - roc_auc_val: 0.62196 - norm_gini: 0.26493 - norm_gini_val: 0.24392          \n",
      "Epoch 00013: norm_gini_val did not improve\n",
      " - 146s - loss: 0.1556 - acc: 0.9636 - val_loss: 0.1551 - val_acc: 0.9636\n",
      "Epoch 15/20\n",
      "roc_auc: 0.63307 - roc_auc_val: 0.62376 - norm_gini: 0.26615 - norm_gini_val: 0.24753          \n",
      "Epoch 00014: norm_gini_val improved from 0.24436 to 0.24753, saving model to keras-kfold-run-01-v1-fold-02-run-01.check\n",
      " - 140s - loss: 0.1553 - acc: 0.9636 - val_loss: 0.1572 - val_acc: 0.9636\n",
      "Epoch 16/20\n",
      "roc_auc: 0.63243 - roc_auc_val: 0.62318 - norm_gini: 0.26487 - norm_gini_val: 0.24635          \n",
      "Epoch 00015: norm_gini_val did not improve\n",
      " - 140s - loss: 0.1552 - acc: 0.9636 - val_loss: 0.1548 - val_acc: 0.9636\n",
      "Epoch 17/20\n",
      "roc_auc: 0.62946 - roc_auc_val: 0.62155 - norm_gini: 0.25893 - norm_gini_val: 0.2431          \n",
      "Epoch 00016: norm_gini_val did not improve\n",
      " - 131s - loss: 0.1551 - acc: 0.9636 - val_loss: 0.1553 - val_acc: 0.9636\n",
      "Epoch 18/20\n",
      "roc_auc: 0.63231 - roc_auc_val: 0.62265 - norm_gini: 0.26461 - norm_gini_val: 0.2453          \n",
      "Epoch 00017: norm_gini_val did not improve\n",
      " - 122s - loss: 0.1549 - acc: 0.9636 - val_loss: 0.1549 - val_acc: 0.9636\n",
      "Epoch 19/20\n",
      "roc_auc: 0.63154 - roc_auc_val: 0.62223 - norm_gini: 0.26309 - norm_gini_val: 0.24447          \n",
      "Epoch 00018: norm_gini_val did not improve\n",
      " - 122s - loss: 0.1549 - acc: 0.9636 - val_loss: 0.1551 - val_acc: 0.9636\n",
      "Epoch 20/20\n",
      "roc_auc: 0.62994 - roc_auc_val: 0.62183 - norm_gini: 0.25988 - norm_gini_val: 0.24367          \n",
      "Epoch 00019: norm_gini_val did not improve\n",
      " - 122s - loss: 0.1547 - acc: 0.9636 - val_loss: 0.1549 - val_acc: 0.9636\n",
      "Epoch 00019: early stopping\n",
      "\n",
      " Fold 2 Run 1 Log-loss: 0.15532\n",
      " Fold 2 Run 1 AUC: 0.62376\n",
      " Fold 2 Run 1 normalized gini: 0.24753\n",
      "\n",
      " Fold 2 Log-loss: 0.15532\n",
      " Fold 2 AUC: 0.62376\n",
      " Fold 2 normalized gini: 0.24753\n",
      "\n",
      " Time taken: 0 hours 46 minutes and 40.84 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Let's split the data into folds. I always use the same random number for reproducibility, \n",
    "# and suggest that you do the same (you certainly don't have to use 1001).\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, random_state=1001)\n",
    "starttime = timer(None)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train, target)):\n",
    "    start_time = timer(None)\n",
    "    X_train, X_val = train[train_index], train[test_index]\n",
    "    y_train, y_val = target[train_index], target[test_index]\n",
    "    train_ids, val_ids = tr_ids[train_index], tr_ids[test_index]\n",
    "    \n",
    "# This is where we define and compile the model. These parameters are not optimal, as they were chosen \n",
    "# to get a notebook to complete in 60 minutes. Other than leaving BatchNormalization and last sigmoid \n",
    "# activation alone, virtually everything else can be optimized: number of neurons, types of initializers, \n",
    "# activation functions, dropout values. The same goes for the optimizer at the end.\n",
    "\n",
    "#########\n",
    "# Never move this model definition to the beginning of the file or anywhere else outside of this loop. \n",
    "# The model needs to be initialized anew every time you run a different fold. If not, it will continue \n",
    "# the training from a previous model, and that is not what you want.\n",
    "#########\n",
    "\n",
    "    # This definition must be within the for loop or else it will continue training previous model\n",
    "    def baseline_model():\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Dense(200, input_dim=X_train.shape[1], kernel_initializer='glorot_normal',\n",
    "                kernel_regularizer=regularizers.l2(1e-4)))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(GaussianDropout(0.5))\n",
    "        \n",
    "        model.add(Dense(120, kernel_initializer='glorot_normal',\n",
    "                        kernel_regularizer=regularizers.l2(5e-5)))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(GaussianDropout(0.4))\n",
    "        \n",
    "        model.add(Dense(80, kernel_initializer='glorot_normal',\n",
    "                       kernel_regularizer=regularizers.l2(2e-5)))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(GaussianDropout(0.35))\n",
    "        \n",
    "        model.add(Dense(40, kernel_initializer='glorot_normal',\n",
    "                  kernel_regularizer=regularizers.l2(1e-5)))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(GaussianDropout(0.3))\n",
    "        \n",
    "        model.add(Dense(15, kernel_initializer='glorot_normal',\n",
    "                 kernel_regularizer=regularizers.l2(5e-6)))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(GaussianDropout(0.2))\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile( optimizer=Adam(lr=STARTING_LR, decay=LR_DECAY),\n",
    "                       metrics = ['accuracy'], loss='binary_crossentropy' )\n",
    "\n",
    "        return model\n",
    "\n",
    "# This is where we repeat the runs for each fold. If you choose runs=1 above, it will run a \n",
    "# regular N-fold procedure.\n",
    "\n",
    "#########\n",
    "# It is important to leave the call to random seed here, so each run starts with a different seed.\n",
    "#########\n",
    "\n",
    "    for run in range(runs):\n",
    "        print('\\n Fold %d - Run %d\\n' % ((i + 1), (run + 1)))\n",
    "        np.random.seed()\n",
    "\n",
    "# Lots to unpack here.\n",
    "\n",
    "# The first callback prints out roc_auc and gini values at the end of each epoch. It must be listed \n",
    "# before the EarlyStopping callback, which monitors gini values saved in the previous callback. Make \n",
    "# sure to set the mode to \"max\" because the default value (\"auto\") will not handle gini properly \n",
    "# (it will act as if the model is not improving even when roc/gini go up).\n",
    "\n",
    "# CSVLogger creates a record of all iterations. Not really needed but it doesn't hurt to have it.\n",
    "\n",
    "# ModelCheckpoint saves a model each time gini improves. Its mode also must be set to \"max\" for reasons \n",
    "# explained above.\n",
    "\n",
    "        callbacks = [\n",
    "            roc_auc_callback(training_data=(X_train, y_train),validation_data=(X_val, y_val)),  # call this before EarlyStopping\n",
    "            EarlyStopping(monitor='norm_gini_val', patience=patience, mode='max', verbose=1),\n",
    "            CSVLogger('keras-kfold-run-01-v1-epochs.log', separator=',', append=False),\n",
    "            ModelCheckpoint(\n",
    "                    'keras-kfold-run-01-v1-fold-' + str('%02d' % (i + 1)) + '-run-' + str('%02d' % (run + 1)) + '.check',\n",
    "                    monitor='norm_gini_val', mode='max', # mode must be set to max or Keras will be confused\n",
    "                    save_best_only=True,\n",
    "                    verbose=1)\n",
    "        ]\n",
    "\n",
    "# The classifier is defined here. Epochs should be be set to a very large number (not 3 like below) which \n",
    "# will never be reached anyway because of early stopping. I usually put 5000 there. Because why not.\n",
    "\n",
    "        nnet = KerasClassifier(\n",
    "            build_fn=baseline_model,\n",
    "# Epoch needs to be set to a very large number ; early stopping will prevent it from reaching\n",
    "            epochs=MAX_EPOCHS,\n",
    "            batch_size=batchsize,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=2,\n",
    "            shuffle=True,\n",
    "            callbacks=callbacks)\n",
    "\n",
    "        fit = nnet.fit(X_train, y_train)\n",
    "        \n",
    "# We want the best saved model - not the last one where the training stopped. So we delete the old \n",
    "# model instance and load the model from the last saved checkpoint. Next we predict values both for \n",
    "# validation and test data, and create a summary of parameters for each run.\n",
    "\n",
    "        del nnet\n",
    "        nnet = load_model('keras-kfold-run-01-v1-fold-' + str('%02d' % (i + 1)) + '-run-' + str('%02d' % (run + 1)) + '.check')\n",
    "        scores_val_run = nnet.predict_proba(X_val, verbose=0)\n",
    "        LL_run = log_loss(y_val, scores_val_run)\n",
    "        print('\\n Fold %d Run %d Log-loss: %.5f' % ((i + 1), (run + 1), LL_run))\n",
    "        AUC_run = roc_auc_score(y_val, scores_val_run)\n",
    "        print(' Fold %d Run %d AUC: %.5f' % ((i + 1), (run + 1), AUC_run))\n",
    "        print(' Fold %d Run %d normalized gini: %.5f' % ((i + 1), (run + 1), AUC_run*2-1))\n",
    "        y_pred_run = nnet.predict_proba(test, verbose=0)\n",
    "        if run > 0:\n",
    "            scores_val = scores_val + scores_val_run\n",
    "            y_pred = y_pred + y_pred_run\n",
    "        else:\n",
    "            scores_val = scores_val_run\n",
    "            y_pred = y_pred_run\n",
    "            \n",
    "# We average all runs from the same fold and provide a parameter summary for each fold. Unless something \n",
    "# is wrong, the numbers printed here should be better than any of the individual runs.\n",
    "\n",
    "    scores_val = scores_val / runs\n",
    "    y_pred = y_pred / runs\n",
    "    LL = log_loss(y_val, scores_val)\n",
    "    print('\\n Fold %d Log-loss: %.5f' % ((i + 1), LL))\n",
    "    AUC = roc_auc_score(y_val, scores_val)\n",
    "    print(' Fold %d AUC: %.5f' % ((i + 1), AUC))\n",
    "    print(' Fold %d normalized gini: %.5f' % ((i + 1), AUC*2-1))\n",
    "    timer(start_time)\n",
    "    \n",
    "# We add up predictions on the test data for each fold. Create out-of-fold predictions for validation data.\n",
    "\n",
    "    if i > 0:\n",
    "        fpred = pred + y_pred\n",
    "        avreal = np.concatenate((avreal, y_val), axis=0)\n",
    "        avpred = np.concatenate((avpred, scores_val), axis=0)\n",
    "        avids = np.concatenate((avids, val_ids), axis=0)\n",
    "    else:\n",
    "        fpred = y_pred\n",
    "        avreal = y_val\n",
    "        avpred = scores_val\n",
    "        avids = val_ids\n",
    "    pred = fpred\n",
    "    cv_LL = cv_LL + LL\n",
    "    cv_AUC = cv_AUC + AUC\n",
    "    cv_gini = cv_gini + (AUC*2-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average Log-loss: 0.15449\n",
      " Out-of-fold Log-loss: 0.15449\n",
      "\n",
      " Average AUC: 0.62459\n",
      " Out-of-fold AUC: 0.61181\n",
      "\n",
      " Average normalized gini: 0.24917\n",
      " Out-of-fold normalized gini: 0.22363\n",
      "\n",
      " Time taken: 1 hours 5 minutes and 30.92 seconds.\n"
     ]
    }
   ],
   "source": [
    "LL_oof = log_loss(avreal, avpred)\n",
    "print('\\n Average Log-loss: %.5f' % (cv_LL/folds))\n",
    "print(' Out-of-fold Log-loss: %.5f' % LL_oof)\n",
    "AUC_oof = roc_auc_score(avreal, avpred)\n",
    "print('\\n Average AUC: %.5f' % (cv_AUC/folds))\n",
    "print(' Out-of-fold AUC: %.5f' % AUC_oof)\n",
    "print('\\n Average normalized gini: %.5f' % (cv_gini/folds))\n",
    "print(' Out-of-fold normalized gini: %.5f' % (AUC_oof*2-1))\n",
    "score = str(round((AUC_oof*2-1), 5))\n",
    "timer(starttime)\n",
    "mpred = pred / folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      " Writing results\n",
      "\n",
      " Writing out-of-fold file:  train_kfold-keras-run-01-v1-oof_0.22363_2017-11-15-00-18.csv\n"
     ]
    }
   ],
   "source": [
    "print('#\\n Writing results')\n",
    "now = datetime.now()\n",
    "oof_result = pd.DataFrame(avreal, columns=['target'])\n",
    "oof_result['prediction'] = avpred\n",
    "oof_result['id'] = avids\n",
    "oof_result.sort_values('id', ascending=True, inplace=True)\n",
    "oof_result = oof_result.set_index('id')\n",
    "sub_file = 'train_kfold-keras-run-01-v1-oof_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n",
    "print('\\n Writing out-of-fold file:  %s' % sub_file)\n",
    "oof_result.to_csv(sub_file, index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " First 10 lines of your k-fold average prediction:\n",
      "\n",
      "      target\n",
      "id          \n",
      "0   0.026075\n",
      "1   0.040622\n",
      "2   0.026526\n",
      "3   0.016326\n",
      "4   0.040718\n",
      "5   0.044725\n",
      "6   0.024835\n",
      "8   0.020163\n",
      "10  0.065100\n",
      "11  0.052962\n",
      "\n",
      " Writing submission:  submission_kfold-average-keras-run-01-v1_0.22363_2017-11-15-00-18.csv\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame(mpred, columns=['target'])\n",
    "result['id'] = te_ids\n",
    "result = result.set_index('id')\n",
    "print('\\n First 10 lines of your k-fold average prediction:\\n')\n",
    "print(result.head(10))\n",
    "sub_file = 'submission_kfold-average-keras-run-01-v1_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n",
    "print('\\n Writing submission:  %s' % sub_file)\n",
    "result.to_csv(sub_file, index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
