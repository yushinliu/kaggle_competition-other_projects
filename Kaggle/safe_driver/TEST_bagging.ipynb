{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = \"C:\\\\Users\\\\hasee\\\\workspace\\\\Kaggle\\\\safe_driver\\\\\" # your folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ind_16_bin ps_ind_17_bin\n",
      "ps_ind_16_bin ps_ind_18_bin\n",
      "ps_ind_17_bin ps_ind_16_bin\n",
      "ps_ind_17_bin ps_ind_18_bin\n",
      "ps_ind_18_bin ps_ind_16_bin\n",
      "ps_ind_18_bin ps_ind_17_bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "train=pd.read_csv(base_path + 'train.csv')\n",
    "test=pd.read_csv(base_path + 'test.csv')\n",
    "\n",
    "train['ps_ind_0609_bin'] = train.apply(lambda x: 1 if x['ps_ind_06_bin'] == 1 else (2 if x['ps_ind_07_bin'] == 1 else \n",
    "(\n",
    "3 if x['ps_ind_08_bin'] == 1 else (4 if x['ps_ind_09_bin'] == 1 else 5)\n",
    "\n",
    ")), axis = 1) #use lambda to define a function more easy and samplified than 'def'\n",
    "\n",
    "test['ps_ind_0609_bin'] = test.apply(lambda x: 1 if x['ps_ind_06_bin'] == 1 else (2 if x['ps_ind_07_bin'] == 1 else \n",
    "(\n",
    "3 if x['ps_ind_08_bin'] == 1 else (4 if x['ps_ind_09_bin'] == 1 else 5)\n",
    "\n",
    ")), axis = 1)\n",
    "\n",
    "train.drop(['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin'], axis = 1, inplace = True)\n",
    "\n",
    "test.drop(['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin'], axis = 1, inplace = True)\n",
    "\n",
    "train['ps_car_13'] = (train['ps_car_13']*train['ps_car_13']* 48400).round(0)#round(0)小数点后一位四舍五入\n",
    "\n",
    "test['ps_car_13'] = (test['ps_car_13']*test['ps_car_13']* 48400).round(0)\n",
    "\n",
    "train['ps_car_12'] = (train['ps_car_12']*train['ps_car_12']).round(4) * 10000#round(4)小数点后三位四舍五入\n",
    "\n",
    "test['ps_car_12'] = (test['ps_car_12']*test['ps_car_12']).round(4) * 10000\n",
    "\n",
    "for c in train[[c for c in train.columns if 'bin' in c]].columns:\n",
    "    for cc in train[[c for c in train.columns if 'bin' in c]].columns:\n",
    "            if train[train[cc] * train[c] == 0].shape[0] == train.shape[0]:\n",
    "                print(c, cc)\n",
    "\n",
    "train['ps_ind_161718_bin'] = train.apply(lambda x: 1 if x['ps_ind_16_bin'] == 1 else\n",
    "                                        (2 if x['ps_ind_17_bin'] == 1 else 3), axis = 1\n",
    "                                        ) #18->1,16->2,17->3\n",
    "\n",
    "test['ps_ind_161718_bin'] = test.apply(lambda x: 1 if x['ps_ind_16_bin'] == 1 else\n",
    "                                        (2 if x['ps_ind_17_bin'] == 1 else 3), axis = 1\n",
    "                                        )\n",
    "\n",
    "train.drop(['ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin'], axis = 1, inplace = True)\n",
    "\n",
    "test.drop(['ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin'], axis = 1, inplace = True)\n",
    "\n",
    "train.to_csv(base_path + '\\\\bagging\\\\train_p.csv', index = False)\n",
    "\n",
    "test.to_csv(base_path + '\\\\bagging\\\\test_p.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.XGBoosting(LB 0.283)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 400\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "# Note: I set EARLY_STOPPING_ROUNDS high so that (when OPTIMIZE_ROUNDS is set)\n",
    "#       I will get lots of information to make my own judgment.  You should probably\n",
    "#       reduce EARLY_STOPPING_ROUNDS if you want to do actual early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import choice\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute gini\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcitons from olivier's kernel\n",
    "# https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]\n",
    "\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv(base_path+'train.csv', na_values=\"-1\") # .iloc[0:200,:]\n",
    "test_df = pd.read_csv(base_path+'test.csv', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from olivier\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.1"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_valid_pred = 0*y\n",
    "y_test_pred = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 0\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 6, 'reg_lambda': 1.1, 'subsample': 0.8, 'max_depth': 4, 'min_child_weight': 4, 'colsample_bytree': 0.82, 'scale_pos_weight': 1.6}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.283418860307994\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28460133503030993\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27493085378002724\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.300541644203868\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28653808001031655\n",
      "\n",
      "Gini for full training set: 0.2857760341406237\n",
      "epoch number: 1\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 8, 'reg_lambda': 1.1, 'subsample': 0.8, 'max_depth': 4, 'min_child_weight': 6, 'colsample_bytree': 0.78, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28643719602939166\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28417896352275973\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27522837603792105\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29768919198090715\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28435038716553107\n",
      "\n",
      "Gini for full training set: 0.28536775387736757\n",
      "epoch number: 2\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 7, 'reg_lambda': 1.2, 'subsample': 0.81, 'max_depth': 5, 'min_child_weight': 6, 'colsample_bytree': 0.8, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28529797977630067\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.283180597312117\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2763343843171666\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29960256760190007\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28465158702108695\n",
      "\n",
      "Gini for full training set: 0.28558594644529056\n",
      "epoch number: 3\n",
      "the parameter is : {'gamma': 9, 'reg_alpha': 6, 'reg_lambda': 1.2, 'subsample': 0.79, 'max_depth': 5, 'min_child_weight': 4, 'colsample_bytree': 0.78, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2847380669360393\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2839046418928033\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2734069280085122\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2981740607112978\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28475110433013895\n",
      "\n",
      "Gini for full training set: 0.28475778961870435\n",
      "epoch number: 4\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 8, 'reg_lambda': 1.2, 'subsample': 0.78, 'max_depth': 4, 'min_child_weight': 5, 'colsample_bytree': 0.74, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2842814873420414\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2834072699130016\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27497803782267194\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2994641567750804\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28447936767789794\n",
      "\n",
      "Gini for full training set: 0.28511395131273254\n",
      "epoch number: 5\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 6, 'reg_lambda': 1.1, 'subsample': 0.81, 'max_depth': 4, 'min_child_weight': 5, 'colsample_bytree': 0.74, 'scale_pos_weight': 1.6}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2838885385958264\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28236763518939567\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27738270091845785\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29967614467878356\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28699597562881896\n",
      "\n",
      "Gini for full training set: 0.28587640852484075\n",
      "epoch number: 6\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 8, 'reg_lambda': 1.1, 'subsample': 0.78, 'max_depth': 5, 'min_child_weight': 6, 'colsample_bytree': 0.8, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28017141813270285\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2821514764459516\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27329580120872665\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2973101865220962\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2836902610153421\n",
      "\n",
      "Gini for full training set: 0.2831264875805348\n",
      "epoch number: 7\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 7, 'reg_lambda': 1.3, 'subsample': 0.79, 'max_depth': 5, 'min_child_weight': 4, 'colsample_bytree': 0.74, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2826710640837786\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2811103043951968\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27236988223281533\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.299024617764033\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2857179021879248\n",
      "\n",
      "Gini for full training set: 0.28396677808494875\n",
      "epoch number: 8\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 7, 'reg_lambda': 1.1, 'subsample': 0.78, 'max_depth': 4, 'min_child_weight': 4, 'colsample_bytree': 0.72, 'scale_pos_weight': 1.6}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2836797801743086\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2823858072886757\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27578113037041463\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29941941511696113\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28528608186434634\n",
      "\n",
      "Gini for full training set: 0.2851179969972386\n",
      "epoch number: 9\n",
      "the parameter is : {'gamma': 10, 'reg_alpha': 7, 'reg_lambda': 1.1, 'subsample': 0.81, 'max_depth': 5, 'min_child_weight': 6, 'colsample_bytree': 0.7, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28685407322490886\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28298720642431086\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27673488218788933\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29840840719278505\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28565995175490455\n",
      "\n",
      "Gini for full training set: 0.2859159992642062\n",
      "epoch number: 10\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 6, 'reg_lambda': 1.3, 'subsample': 0.81, 'max_depth': 4, 'min_child_weight': 5, 'colsample_bytree': 0.82, 'scale_pos_weight': 1.6}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2848945874768932\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28374246401927383\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27487301261119235\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2999546815109927\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2841303331157925\n",
      "\n",
      "Gini for full training set: 0.2852917770540049\n",
      "epoch number: 11\n",
      "the parameter is : {'gamma': 10, 'reg_alpha': 6, 'reg_lambda': 1.3, 'subsample': 0.8, 'max_depth': 5, 'min_child_weight': 4, 'colsample_bytree': 0.82, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28360589485153065\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2833792289035546\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.275221405042126\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2997753146701827\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2838829278056322\n",
      "\n",
      "Gini for full training set: 0.28496308978220763\n",
      "epoch number: 12\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 8, 'reg_lambda': 1.1, 'subsample': 0.8, 'max_depth': 5, 'min_child_weight': 5, 'colsample_bytree': 0.78, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2821013875672468\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28200691496241614\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2728647245850042\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29843876993722807\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2827585942076126\n",
      "\n",
      "Gini for full training set: 0.28340784060080715\n",
      "epoch number: 13\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 7, 'reg_lambda': 1.3, 'subsample': 0.79, 'max_depth': 5, 'min_child_weight': 6, 'colsample_bytree': 0.8, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28436346279789715\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28384893305094816\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27432544718583973\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.3006098225520152\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2852599345713692\n",
      "\n",
      "Gini for full training set: 0.2854570756084034\n",
      "epoch number: 14\n",
      "the parameter is : {'gamma': 10, 'reg_alpha': 7, 'reg_lambda': 1.1, 'subsample': 0.78, 'max_depth': 5, 'min_child_weight': 4, 'colsample_bytree': 0.76, 'scale_pos_weight': 1.6}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28325499790097053\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2843492620531549\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27595000785771273\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.3000997611154419\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28399899666713013\n",
      "\n",
      "Gini for full training set: 0.2853230918866194\n",
      "epoch number: 15\n",
      "the parameter is : {'gamma': 10, 'reg_alpha': 8, 'reg_lambda': 1.1, 'subsample': 0.78, 'max_depth': 5, 'min_child_weight': 5, 'colsample_bytree': 0.76, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2834528823146766\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2818785420610741\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2750594828147408\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29923521718984636\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.286221720564676\n",
      "\n",
      "Gini for full training set: 0.28497595662782904\n",
      "epoch number: 16\n",
      "the parameter is : {'gamma': 10, 'reg_alpha': 6, 'reg_lambda': 1.3, 'subsample': 0.77, 'max_depth': 4, 'min_child_weight': 4, 'colsample_bytree': 0.82, 'scale_pos_weight': 1.6}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2846982881016916\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28220161196985794\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2748771000065823\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.30049747018818007\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28496279860592566\n",
      "\n",
      "Gini for full training set: 0.28521270327587056\n",
      "epoch number: 17\n",
      "the parameter is : {'gamma': 9, 'reg_alpha': 8, 'reg_lambda': 1.2, 'subsample': 0.77, 'max_depth': 5, 'min_child_weight': 5, 'colsample_bytree': 0.72, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28376748429724685\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2827930975763323\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27634734281612205\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29803290151500805\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28524072709073944\n",
      "\n",
      "Gini for full training set: 0.2850488430870536\n",
      "epoch number: 18\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 6, 'reg_lambda': 1.2, 'subsample': 0.78, 'max_depth': 4, 'min_child_weight': 4, 'colsample_bytree': 0.7, 'scale_pos_weight': 1.6}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2843917682063185\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2824624830013296\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2749117858248247\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29812184169136613\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2868824426987494\n",
      "\n",
      "Gini for full training set: 0.2851326003944221\n",
      "epoch number: 19\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 7, 'reg_lambda': 1.2, 'subsample': 0.79, 'max_depth': 5, 'min_child_weight': 4, 'colsample_bytree': 0.78, 'scale_pos_weight': 1.7}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "  Gini =  0.2826196669976514\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2792279263986658\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27183530284421187\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29967983231270845\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28493981922728806\n",
      "\n",
      "Gini for full training set: 0.283405486619406\n",
      "epoch number: 20\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 6, 'reg_lambda': 1.1, 'subsample': 0.79, 'max_depth': 5, 'min_child_weight': 4, 'colsample_bytree': 0.8, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28325063703361286\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2768253112161633\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27058961500700873\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2972336405527837\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2832006822535519\n",
      "\n",
      "Gini for full training set: 0.281995047159476\n",
      "epoch number: 21\n",
      "the parameter is : {'gamma': 9, 'reg_alpha': 8, 'reg_lambda': 1.2, 'subsample': 0.79, 'max_depth': 4, 'min_child_weight': 5, 'colsample_bytree': 0.8, 'scale_pos_weight': 1.6}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2845538122442961\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2828109046111179\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2753135307642083\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29926855699423527\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28482080644539376\n",
      "\n",
      "Gini for full training set: 0.2851519766962606\n",
      "epoch number: 22\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 8, 'reg_lambda': 1.1, 'subsample': 0.79, 'max_depth': 4, 'min_child_weight': 6, 'colsample_bytree': 0.82, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28406524625144125\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.282895185834073\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2764219308975415\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.30081995642935966\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2838630563429565\n",
      "\n",
      "Gini for full training set: 0.28540805126124547\n",
      "epoch number: 23\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 6, 'reg_lambda': 1.1, 'subsample': 0.81, 'max_depth': 4, 'min_child_weight': 6, 'colsample_bytree': 0.82, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2851174784879198\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28279235933479907\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27597628678091324\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.3003323760833254\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2838231574218233\n",
      "\n",
      "Gini for full training set: 0.2853847982201655\n",
      "epoch number: 24\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 7, 'reg_lambda': 1.1, 'subsample': 0.77, 'max_depth': 4, 'min_child_weight': 5, 'colsample_bytree': 0.72, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2854806293148374\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28384556228878255\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27576251088785353\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29946328693452007\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2848866806641548\n",
      "\n",
      "Gini for full training set: 0.2856807888767665\n",
      "epoch number: 25\n",
      "the parameter is : {'gamma': 9, 'reg_alpha': 6, 'reg_lambda': 1.3, 'subsample': 0.81, 'max_depth': 5, 'min_child_weight': 4, 'colsample_bytree': 0.76, 'scale_pos_weight': 1.7}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28390355220927554\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28077898402883805\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2716523024026788\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29893212063402186\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2837141363697525\n",
      "\n",
      "Gini for full training set: 0.2835621270103088\n",
      "epoch number: 26\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 8, 'reg_lambda': 1.1, 'subsample': 0.81, 'max_depth': 4, 'min_child_weight': 5, 'colsample_bytree': 0.78, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28461003685886355\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2820204466863443\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2756137477919214\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.299633371392261\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28594737998284525\n",
      "\n",
      "Gini for full training set: 0.2853417654018773\n",
      "epoch number: 27\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 6, 'reg_lambda': 1.1, 'subsample': 0.8, 'max_depth': 4, 'min_child_weight': 5, 'colsample_bytree': 0.76, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2846063478225471\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28156003545847097\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2737535226149096\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29988766295025926\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2840760385839167\n",
      "\n",
      "Gini for full training set: 0.28455784699727427\n",
      "epoch number: 28\n",
      "the parameter is : {'gamma': 11, 'reg_alpha': 8, 'reg_lambda': 1.2, 'subsample': 0.79, 'max_depth': 4, 'min_child_weight': 4, 'colsample_bytree': 0.82, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.28422534318628134\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28280541241861223\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27595921728274453\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.30097729547682395\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2843097922632172\n",
      "\n",
      "Gini for full training set: 0.28543126556226006\n",
      "epoch number: 29\n",
      "the parameter is : {'gamma': 8, 'reg_alpha': 8, 'reg_lambda': 1.1, 'subsample': 0.8, 'max_depth': 4, 'min_child_weight': 5, 'colsample_bytree': 0.78, 'scale_pos_weight': 1.8}\n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2844422118555787\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2825795307907989\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27614939722051135\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2994561771109726\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.2845992524362141\n",
      "\n",
      "Gini for full training set: 0.2852401285094709\n"
     ]
    }
   ],
   "source": [
    "# Run CV\n",
    "#run the model\n",
    "n_epochs=30\n",
    "for epoch in range(n_epochs):\n",
    "    y_valid_pred = 0*y\n",
    "    y_test_pred = 0\n",
    "    print(\"epoch number:\",epoch)\n",
    "    param_grid={\n",
    "    'gamma':choice(range(8,12,1)),\n",
    "    'reg_alpha':choice(range(6,9,1)),\n",
    "    'reg_lambda':choice([1.1,1.2,1.3]),\n",
    "    'subsample':choice([0.77,0.78,0.79,0.80,0.81]),\n",
    "    'max_depth':choice(range(4,6,1)),\n",
    "    'min_child_weight':choice(range(4,7,1)),\n",
    "    'colsample_bytree':choice([0.70,0.72,0.74,0.76,0.78,0.80,0.82]),\n",
    "    'scale_pos_weight':choice([1.6,1.7,1.8])\n",
    "}\n",
    "    print(\"the parameter is :\",param_grid)\n",
    "    model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=param_grid['max_depth'],\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=param_grid['subsample'],\n",
    "                        min_child_weight=param_grid['min_child_weight'],\n",
    "                        colsample_bytree=param_grid['colsample_bytree'],\n",
    "                        scale_pos_weight=param_grid['scale_pos_weight'],\n",
    "                        gamma=param_grid['gamma'],\n",
    "                        reg_alpha=param_grid['reg_alpha'],\n",
    "                        reg_lambda=param_grid['reg_lambda'],\n",
    "                     )\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "        # Create data for this fold\n",
    "        y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "        X_test = test_df.copy()\n",
    "        print( \"\\nFold \", i)\n",
    "    \n",
    "        # Enocode data\n",
    "        for f in f_cats:\n",
    "            X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "    \n",
    "        # Run model for this fold\n",
    "        if OPTIMIZE_ROUNDS:\n",
    "            eval_set=[(X_valid,y_valid)]\n",
    "            fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric=gini_xgb,\n",
    "                               early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                               verbose=False\n",
    "                                 )\n",
    "            print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "            print( \"  Best gini = \", model.best_score )\n",
    "        else:\n",
    "            fit_model = model.fit( X_train, y_train )\n",
    "        \n",
    "        # Generate validation predictions for this fold\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "        print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "        y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "        # Accumulate test set predictions\n",
    "        y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "        del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "    y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "    print( \"\\nGini for full training set:\",eval_gini(y, y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the gridsearchCV\n",
    "\n",
    "# Set up classifier\n",
    "model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=5,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=.81,\n",
    "                        min_child_weight=6,\n",
    "                        colsample_bytree=.7,\n",
    "                        scale_pos_weight=1.7,\n",
    "                        gamma=10,\n",
    "                        reg_alpha=7,\n",
    "                        reg_lambda=1.1,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "  Gini =  0.28685407322490886\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28298720642431086\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27673488218788933\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29840840719278505\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28565995175490455\n",
      "\n",
      "Gini for full training set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2859159992642062"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run CV\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    # Enocode data\n",
    "    for f in f_cats:\n",
    "        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "    # Run model for this fold\n",
    "    if OPTIMIZE_ROUNDS:\n",
    "        eval_set=[(X_valid,y_valid)]\n",
    "        fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric=gini_xgb,\n",
    "                               early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                               verbose=False\n",
    "                             )\n",
    "        print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "        print( \"  Best gini = \", model.best_score )\n",
    "    else:\n",
    "        fit_model = model.fit( X_train, y_train )\n",
    "        \n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    # Accumulate test set predictions\n",
    "    y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "eval_gini(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save validation predictions for stacking/ensembling\n",
    "val = pd.DataFrame()\n",
    "val['id'] = id_train\n",
    "val['target'] = y_valid_pred.values\n",
    "val.to_csv(base_path+\"\\\\bagging\\\\xgb_valid.csv\", float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_test_pred\n",
    "sub.to_csv(base_path+\"\\\\bagging\\\\xgb_test.csv\", float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
