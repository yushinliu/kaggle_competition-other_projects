{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ML Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 为什么要使用特征选择（feature selection）？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "feature selection is a method to take a subset of relevent features, it will help you by choosing the features that will give you as good or better accuracy whilst requiring less data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2. What is the effect on the coefficients of logistic regression if two predictors are highly correlated? What are the confidence intervals of the coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "highly correlated: highly correlated means one feature increase and the correlated one will increase as well, the points distributed around a straight line.\n",
    "confidence interval: the estimate to the coefficient of samples, e.g. the probability tht coefficients belongs to [1,2] is 0.95\n",
    "coefficients of logistic regression:Coefficient of the features in the decision function and this time the p-value is 0.05\n",
    "\n",
    "Answer: increase the variance,the confidence intervals of coefficients will be less significant(embrace null value e.g. risk ratio=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.What’s the difference between Gaussian Mixture Model and K-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the difference between Gaussian Mixture Model and K-Means is that Gaussian Mixture Model is Statistical Model(which introduced probability), one cluster in GMM is a single Statistical Model and sum up all the weighted models is GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.How do you pick k for K-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. mean squared: $K=\\sqrt{m/2}$\n",
    "2. Elbow method: calculate the optimization cost function, $$ J(c^{(1)},...,c^{(m)},\\mu_{1},...,\\mu_{k})=\\frac{1}{m}\\sum_{1}^{m}(||x^{i}-\\mu_{c^{(i)}}||)$$, and $c^{(i)}$ indicates the centers most close to the $x^{(i)}$ and $\\mu_{k}$ is the cluster center, different K determine different J ,and we plot the K-J and found the elbow point of the curve\n",
    "3. Graph method: looking at graph clusters and determined the centers mannully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.How do you know when Gaussian Mixture Model is applicable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can perform GMM when you know that the data points are mixtures of a gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Assuming a clustering model’s labels are known, how do you evaluate the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model expects a scored dataset as input (or 2 in case you would like to compare the performance of 2 different models). This means that you need to train your model using the Train Model module and make predictions on some dataset using the Score Model module, before you can evaluate the results. The evaluation is based on the scored labels/probabilities along with the true labels, all of which are output by the Score Model module.\n",
    "\n",
    "you can also use cross validation , it can get more reasonable result, you can set up the metric which means that in which way you want to evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What’s an example of a machine learning project you’re proud of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Choose any machine learning algorithm and describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Describe how Gradient Boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting works sequentially adding the predictors to an ensemble, each one correct its predecessor , the method tries to fit new predictor to the residual errors made by previous predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  Describe the decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.What is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a simple neural network has input layer , hidden layer and output layer, the input after multiply by weights and cross added to the next layer ,there are also activation function in the hidden and output layers to ensure the nonlinearity of the NN, we can update the weights by implement back propagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Explain the Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing a model’s complexity will typically increase its variance and reduce its bias.(more freedom of model,multi-kernel)\n",
    "Conversely, reducing a model’s complexity increases its bias and reduces its variance.\n",
    "This is why it is called a tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.How do you deal with unbalanced binary classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we can use resampling method( with replacement or without replacement)\n",
    "2. try other metrics like the ROC and F1score instead of the accuracy,Use Penalized Models. Accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of the values.\n",
    "3. Use Penalized Models. Like penalized-SVM and penalized-LDA. They put additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model towards paying attention to minority class.\n",
    "4. Choose the algorithm that is insensitive to imbalance data, like cost-sensitive learning. The basic idea is to adjust the cost of various classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.What’s the difference between L1 and L2 regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 9.What sort features could you give an Uber driver to predict if they will accept a ride request or not? What supervised learning algorithm would you use to solve the problem and how would compare the results of the algorithm?|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. features: the weather, the distances between driver and customers, the time cost from drive to customers( including traffic jam), the condition of road( e.g. set up to 5 classes)\n",
    "2. it's a binary classification problem, I can use logistic classification or decision tree classification, or more complex we shall use random forest or XGBoosting which has high performance in the problem.\n",
    "3. you can caculate the F1scores of each algorithm and make a comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Name and describe three different kernel functions and in what situation you would use each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Kernel :$k(x,y)=x^{T}y+1$ , can be used in the normal cases\n",
    "2. polynomial kernel : $k(x,y)=(\\alpha x^{T}y+1)^{d}$, the polynomial kernel will suited the problem when data are nomalized\n",
    "3. RBF kernel:$k(x,y)=exp(-\\frac{||x-y||^{2}}{2\\sigma^{2}})$\n",
    "Technically if you use squared exponential kernel, than you're method is nonparametric, if you use polynomial kernels, you're model is parametric. In a way nonparametric model means that the complexity of the model is potentially infinite, it's complexity can grow with the data. If you give it more and more data, it will be able to represent more and more complex relationships. In contrast a parametric model's size is fixed, so after a certain point your model will be saturated, and giving it more and more data won't help. So asymptotically, assuming you have unlimited data and very weak assumptions about the problem, a nonparametric method is always better.\n",
    "\n",
    "In conclusion, the squared exponential kernel is generally more flexible than the linear or polynomial kernels in that you can model a whole lot more functions with its function space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.How do you deal with sparse data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. you can handling with the data first "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
